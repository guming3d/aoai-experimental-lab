# Import dependencies    
import datetime    
import json    
import time    
import os    
import shutil
from openai import AzureOpenAI    
from dotenv import load_dotenv    
import copy    
import textwrap    
import requests    
import threading    
import streamlit as st    
from queue import Queue  
from PIL import Image
import base64
import io
import fitz  # PyMuPDF
import pandas as pd
from random import randint
from process_inputs import process_inputs  
# Load environment variables    
load_dotenv("./.env")    

# Define a flag to toggle deletion of the temporary folder
DELETE_TEMP_FOLDER = os.getenv("DELETE_TEMP_FOLDER", "true").lower() == "true"
TEMP_FOLDER = "./use-cases/Custom Scenario/images"

import os
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Get the value of the 'debug_mode' environment variable, default to 'true' if not set
offline_mode = os.getenv('offline_mode', 'true')
# Print the value
print(f"offline_mode: {offline_mode}")
# Log the value
logging.info(f"offline_mode: {offline_mode}")


offline_message="æ­¤åŠŸèƒ½åœ¨ç¦»çº¿æ¨¡å¼ä¸‹è¢«ç¦ç”¨ã€‚è¯·åœ¨æœ¬åœ°æ‰˜ç®¡æ­¤åº”ç”¨ç¨‹åºå¹¶ä½¿ç”¨æ‚¨è‡ªå·±çš„ API å¯†é’¥è¿›è¡Œå®æ—¶å°è¯•ï¼è”ç³» luca.stamatescu@microsoft.com äº†è§£æ›´å¤šä¿¡æ¯ã€‚"

# Function to read XML file content as a string  
def load_use_case_from_file(file_path):  
    with open(file_path, 'r') as file:  
        return file.read()  
    
def get_csv_data(use_case,column_name):
    # Load the CSV file into a DataFrame
    csv_file_path = './o1-vs-4o-scenarios.csv'
    df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
    row = df[df['Use Case'] == use_case]
    if not row.empty:
        return row.iloc[0][column_name]
    else:
        return f"Error - {column_name} not found."

def save_csv_data(use_case, column_name, value):
    # Check if debug_mode is true
    if os.getenv('debug_mode') == 'true':
        # Load the CSV file into a DataFrame
        csv_file_path = './o1-vs-4o-scenarios.csv'
        df = pd.read_csv(csv_file_path)
        
        # Check if the use case already exists
        if use_case in df['Use Case'].values:
            df.loc[df['Use Case'] == use_case, column_name] = value
            # Save the updated DataFrame back to the CSV file
            df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')
        else:
            return f"Error - Use case '{use_case}' not found."


# Define function for calling GPT4o with streaming  
def gpt4o_call(system_message, user_message, result_dict, queue, selected_use_case):    
    if offline_mode == 'true':
        response_text = get_csv_data(selected_use_case, 'gpt4o')
        for i in range(0, len(response_text), 50):  # Simulate streaming
            queue.put(response_text[:i+50])
            time.sleep(0.2)
        result_dict['4o'] = {
            'response': response_text,
            'time': get_csv_data(selected_use_case, 'gpt4o_time')
        }
    else:
        client = AzureOpenAI(    
            api_version=os.getenv("4oAPI_VERSION"),    
            azure_endpoint=os.getenv("4oAZURE_ENDPOINT"),    
            api_key=os.getenv("4oAPI_KEY")    
        )    
        
        start_time = time.time()    
        
        completion = client.chat.completions.create(    
            model=os.getenv("4oMODEL"),    
            messages=[    
                {"role": "system", "content": system_message},    
                {"role": "user", "content": user_message},    
            ],    
            stream=True  # Enable streaming  
        )    
        
        response_text = ""  
        for chunk in completion:  
            if chunk.choices and chunk.choices[0].delta.content:  
                response_text += chunk.choices[0].delta.content  
                queue.put(response_text)  
        
        elapsed_time = time.time() - start_time    
        
        result_dict['4o'] = {    
            'response': response_text,    
            'time': elapsed_time    
        }    
        queue.put(f"Elapsed time: {elapsed_time:.2f} seconds")    

def o1_call(system_message, user_message):
    client = AzureOpenAI(    
        api_version=os.getenv("o1API_VERSION"),    
        azure_endpoint=os.getenv("o1AZURE_ENDPOINT"),    
        api_key=os.getenv("o1API_KEY")    
    )    
    
    start_time = time.time()    
    
    prompt = system_message + user_message

    completion = client.chat.completions.create(    
        model=os.getenv("o1API_MODEL"),    
        messages=[      
            {"role": "user", "content": prompt},    
        ],    
    )
    elapsed_time = time.time() - start_time   
    messageo1=completion.choices[0].message.content 
    return messageo1, elapsed_time  

# Define function for calling O1 and storing the result  
def o1_call_simultaneous_handler(system_message, user_message, result_dict,selected_use_case ):    
    if offline_mode == 'true':
        response = get_csv_data(selected_use_case, 'o1')
        # Sleep for the time taken by o1
        o1_time_elapsed = get_csv_data(selected_use_case, 'o1_time')
        time.sleep(o1_time_elapsed)
        print("SLEPT FOR ", o1_time_elapsed)
        result_dict['o1'] = {
            'response': response,
            'time': get_csv_data(selected_use_case, 'o1_time')
        }
    else:
        response,elapsed_time=o1_call(system_message, user_message)
        
        result_dict['o1'] = {    
            'response': response,    
            'time': elapsed_time    
        }    
  
# Define function for comparing responses using O1  
def compare_responses(response_4o, response_o1):  
    system_message = "You are an expert reviewer, who is helping review two candidates responses to a question."  
    user_message = f"Compare the following two responses and summarize the key differences:\n\nResponse 1 GPT-4o Model:\n{response_4o}\n\nResponse 2 o1 Model:\n{response_o1}. Generate a succinct comparison, and call out the key elements that make one response better than another. Be critical in your analysis."  
    comparison_result, _ = o1_call(system_message, user_message)  
      
    return comparison_result  

# Define function for comparing responses using O1  
def compare_responses_simple(response_4o, response_o1):  
    system_message = "You are an expert reviewer, who is helping review two candidates responses to a question."  
    user_message = f"Compare the following two responses and summarize the key differences:\n\nResponse 1 GPT-4o Model:\n{response_4o}\n\nResponse 2 o1 Model:\n{response_o1}. Generate a succinct comparison, and call out the key elements that make one response better than another. Be succinct- only use 3 sentences."  
    comparison_result, _ = o1_call(system_message, user_message)  
      
    return comparison_result  

# Function to process images and convert them to text using GPT-4o
def process_images(images):
    client = AzureOpenAI(
        api_version=os.getenv("4oAPI_VERSION"),
        azure_endpoint=os.getenv("4oAZURE_ENDPOINT"),
        api_key=os.getenv("4oAPI_KEY")
    )

    descriptions = []
    for image in images:
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        system_prompt = "Generate a highly detailed text description of this image, making sure to capture all the information within the image as words. If there is text, tables or other text based information, include this in a section of your response as markdown."
        response = client.chat.completions.create(
            model=os.getenv("4oMODEL"),
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": [
                    {"type": "text", "text": "Here is the input image:"},
                    {"type": "image_url", "image_url": {"url": f'data:image/jpg;base64,{img_str}', "detail": "low"}}
                ]}
            ],
            temperature=0,
        )
        descriptions.append(response.choices[0].message.content)
    
    return descriptions

def process_pdf(pdf_path, output_folder):
    pdf_document = fitz.open(pdf_path)
    for page_num in range(len(pdf_document)):
        page = pdf_document.load_page(page_num)
        pix = page.get_pixmap()
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        image_path = os.path.join(output_folder, f"{os.path.splitext(os.path.basename(pdf_path))[0]}_page_{page_num + 1}.jepg")
        img.save(image_path, "JPEG")

def load_images_and_descriptions(selected_title):
    use_case_folder = f"./use-cases/{selected_title}/images"

    if os.path.exists(use_case_folder):
        image_files = [os.path.join(use_case_folder, f) for f in os.listdir(use_case_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        descriptions = []
        for img_file in image_files:
            base_name = os.path.splitext(os.path.basename(img_file))[0]
            description_path = os.path.join(use_case_folder, f"{base_name}.txt")
            if os.path.exists(description_path):
                with open(description_path, 'r', encoding='utf-8') as f:
                    description = f.read()
                    descriptions.append((img_file, description))
    else:
        image_files = []
        descriptions = []

    if descriptions:
        st.session_state['descriptions'] = descriptions
    else:
        st.session_state['descriptions'] = []

# Streamlit app    
def main():

    def render_images_and_descriptions():  
        cols = st.columns(3)  # Adjust the number of columns as needed  
        fixed_height = 200  # Fixed height for images in pixels  
    
        for i, (img_path, description) in enumerate(st.session_state['descriptions']):  
            image = Image.open(img_path)  
            col = cols[i % 3]  
            with col.container():  
                buffered = io.BytesIO()  
                image.save(buffered, format="JPEG")  
                img_str = base64.b64encode(buffered.getvalue()).decode()  
                # Generate a unique key based on description content  
                unique_id = f"{i}"  
                col.markdown(  
                    f"""  
                    <style>  
                        .image-container-{unique_id} {{  
                            height: {fixed_height}px;  
                            display: flex;  
                            align-items: center;  
                            justify-content: center;  
                            overflow: hidden;  
                            margin-bottom: 5px;  
                        }}  
                        .image-container-{unique_id} img {{  
                            height: 100%;  
                            width: auto;  
                            object-fit: cover;  
                        }}  
                    </style>  
                    <div class='image-container-{unique_id}'>  
                        <img src='data:image/jpeg;base64,{img_str}' alt='Image'>  
                    </div>  
                    """,  
                    unsafe_allow_html=True  
                )  
                # Use st.text_area for the description with a unique key  
                col.text_area("Description", description, height=100, key=f"description_{i}")  

    st.set_page_config(page_title="Azure OpenAI Studio å®éªŒå®¤ | GPT-4o ä¸ 4o æ¯”è¾ƒå·¥å…·",page_icon="./favicon.ico", layout="wide")
    selected_item = st.sidebar.empty()

    def set_selected_item(item):
        st.session_state.selected_title = item
        load_images_and_descriptions(item)

    # Create two columns
    col1, col2 = st.sidebar.columns([8, 3])  # Adjust the ratio as needed

    # Place the title in the first column
    with col1:
        st.title("Azure OpenAI å®éªŒå®¤ ğŸ”¬")

    # Place the image in the second column
    with col2:
        st.text("")
        st.text("")
        st.image("./azureopenaistudio.png", width=50)
 
    st.sidebar.subheader("GPT-4o ä¸ o1-preview æ¯”è¾ƒå·¥å…·") 

    st.sidebar.markdown("---")  

    # Custom CSS to make buttons full width and prevent wrapping
    st.sidebar.markdown("""
        <style>
        .stButton button {
            width: 100%;
            white-space: nowrap;
        }
        </style>
    """, unsafe_allow_html=True)

    # # Hidden text input to store the selected item
    # st.sidebar.text_input("", key="selected_item", on_change=lambda: set_selected_item(st.session_state.selected_item))
    if st.sidebar.button("Custom Scenario", key="custom_1"):
        if offline_mode == 'true':
            st.toast(offline_message, icon="âš ï¸")
        else:
            set_selected_item("Custom Scenario")
            print(offline_mode)
    st.sidebar.markdown("---") 
    

    
    
    # Insurance section
    st.sidebar.header("ä¿é™©")  
    if st.sidebar.button("å®¶åº­ä¿é™©ç´¢èµ” â­ï¸", key="insurance_1"):
        # set_selected_item("å®¶åº­ä¿é™©ç´¢èµ”")
        set_selected_item("Home Insurance Claim")
    if st.sidebar.button("æ±½è½¦ä¿é™©ç´¢èµ” â­ï¸", key="insurance_2"):
        # set_selected_item("æ±½è½¦ä¿é™©ç´¢èµ”")
        set_selected_item("Auto Insurance Claim")
    if st.sidebar.button("å®¢æˆ·æœåŠ¡å’Œä¿ç•™", key="insurance_3"):
        # set_selected_item("å®¢æˆ·æœåŠ¡å’Œä¿ç•™")
        set_selected_item("Customer Service and Retention")
    if st.sidebar.button("äº§å“å¼€å‘å’Œåˆ›æ–°", key="insurance_4"):
        # set_selected_item("äº§å“å¼€å‘å’Œåˆ›æ–°")
        set_selected_item("Product Development and Innovation")
    if st.sidebar.button("é£é™©ç®¡ç†å’Œåˆè§„", key="insurance_5"):
        # set_selected_item("é£é™©ç®¡ç†å’Œåˆè§„")
        set_selected_item("Risk Management and Compliance")
    st.sidebar.markdown("---")  

    # Banking section
    st.sidebar.header("é“¶è¡Œ")  
    if st.sidebar.button("ä¿¡ç”¨é£é™©è¯„ä¼°å’Œç®¡ç† â­ï¸", key="banking_1"):
        # set_selected_item("ä¿¡ç”¨é£é™©è¯„ä¼°å’Œç®¡ç†")
        set_selected_item("Credit Risk Assessment and Management")
    if st.sidebar.button("æ¬ºè¯ˆæ£€æµ‹å’Œé¢„é˜²", key="banking_2"):
        # set_selected_item("æ¬ºè¯ˆæ£€æµ‹å’Œé¢„é˜²")
        set_selected_item("Fraud Detection and Prevention")
    if st.sidebar.button("åˆè§„å’ŒæŠ¥å‘Š", key="banking_3"):
        # set_selected_item("åˆè§„å’ŒæŠ¥å‘Š")
        set_selected_item("Regulatory Compliance and Reporting")
    if st.sidebar.button("å®¢æˆ·å…³ç³»ç®¡ç†", key="banking_4"):
        # set_selected_item("å®¢æˆ·å…³ç³»ç®¡ç†")
        set_selected_item("Customer Relationship Management")
    if st.sidebar.button("æŠ•èµ„å’ŒæŠ•èµ„ç»„åˆç®¡ç†", key="banking_5"):
        # set_selected_item("æŠ•èµ„å’ŒæŠ•èµ„ç»„åˆç®¡ç†")
        set_selected_item("Investment and Portfolio Management")
    st.sidebar.markdown("---") 

    # Retail section
    st.sidebar.header("é›¶å”®")  
    if st.sidebar.button("åº“å­˜å’Œä¾›åº”é“¾ç®¡ç†", key="retail_1"):
        # set_selected_item("åº“å­˜å’Œä¾›åº”é“¾ç®¡ç†")
        set_selected_item("Inventory and Supply Chain Management")
    if st.sidebar.button("å•†å“å’Œå®šä»·", key="retail_2"):
        # set_selected_item("å•†å“å’Œå®šä»·")
        set_selected_item("Merchandising and Pricing")
    if st.sidebar.button("å®¢æˆ·ç»†åˆ†å’Œä¸ªæ€§åŒ–", key="retail_3"):
        # set_selected_item("å®¢æˆ·ç»†åˆ†å’Œä¸ªæ€§åŒ–")
        set_selected_item("Customer Segmentation and Personalization")
    if st.sidebar.button("å…¨æ¸ é“å’Œç”µå­å•†åŠ¡", key="retail_4"):
        # set_selected_item("å…¨æ¸ é“å’Œç”µå­å•†åŠ¡")
        set_selected_item("Omnichannel and E-commerce")
    if st.sidebar.button("å¿ è¯šåº¦å’Œä¿ç•™", key="retail_5"):
        # set_selected_item("å¿ è¯šåº¦å’Œä¿ç•™")
        set_selected_item("Loyalty and Retention")
    st.sidebar.markdown("---")  



    # Utilities section
    st.sidebar.header("å…¬ç”¨äº‹ä¸š")  
    if st.sidebar.button("éœ€æ±‚å’Œä¾›åº”ç®¡ç†", key="utilities_1"):
        # set_selected_item("éœ€æ±‚å’Œä¾›åº”ç®¡ç†")
        set_selected_item("Demand and Supply Management")
    if st.sidebar.button("èµ„äº§å’Œç½‘ç»œç®¡ç†", key="utilities_2"):
        # set_selected_item("èµ„äº§å’Œç½‘ç»œç®¡ç†")
        set_selected_item("Asset and Network Management")
    if st.sidebar.button("å®¢æˆ·æœåŠ¡å’Œè®¡è´¹", key="utilities_3"):
        # set_selected_item("å®¢æˆ·æœåŠ¡å’Œè®¡è´¹")
        set_selected_item("Customer Service and Billing")
    if st.sidebar.button("èƒ½æºæ•ˆç‡å’Œå¯æŒç»­æ€§", key="utilities_4"):
        # set_selected_item("èƒ½æºæ•ˆç‡å’Œå¯æŒç»­æ€§")
        set_selected_item("Energy Efficiency and Sustainability")
    if st.sidebar.button("åˆè§„å’ŒæŠ¥å‘Š", key="utilities_5"):
        # set_selected_item("åˆè§„å’ŒæŠ¥å‘Š")
        set_selected_item("Regulatory Compliance and Reporting")
    st.sidebar.markdown("---")  

    # Mining section
    st.sidebar.header("é‡‡çŸ¿")  
    if st.sidebar.button("å‹˜æ¢å’Œå¯è¡Œæ€§", key="mining_1"):
        # set_selected_item("å‹˜æ¢å’Œå¯è¡Œæ€§")
        set_selected_item("Exploration and Feasibility")
    if st.sidebar.button("çŸ¿å±±è§„åˆ’å’Œè®¾è®¡", key="mining_2"):
        # set_selected_item("çŸ¿å±±è§„åˆ’å’Œè®¾è®¡")
        set_selected_item("Mine Planning and Design")
    if st.sidebar.button("ç”Ÿäº§å’ŒåŠ å·¥", key="mining_3"):
        # set_selected_item("ç”Ÿäº§å’ŒåŠ å·¥")
        set_selected_item("Production and Processing")
    if st.sidebar.button("ç¯å¢ƒå’Œç¤¾ä¼šå½±å“", key="mining_4"):
        # set_selected_item("ç¯å¢ƒå’Œç¤¾ä¼šå½±å“")
        set_selected_item("Environmental and Social Impact")
    if st.sidebar.button("å¥åº·å’Œå®‰å…¨", key="mining_5"):
        # set_selected_item("å¥åº·å’Œå®‰å…¨")
        set_selected_item("Health and Safety")
    st.sidebar.markdown("---")  

    # Telecommunications section
    st.sidebar.header("ç”µä¿¡")  
    if st.sidebar.button("ç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–", key="telecom_1"):
        # set_selected_item("ç½‘ç»œè§„åˆ’å’Œä¼˜åŒ–")
        set_selected_item("Network Planning and Optimization")
    if st.sidebar.button("æœåŠ¡å¼€å‘å’Œåˆ›æ–°", key="telecom_2"):
        # set_selected_item("æœåŠ¡å¼€å‘å’Œåˆ›æ–°")
        set_selected_item("Service Development and Innovation")
    if st.sidebar.button("å®¢æˆ·è·å–å’Œä¿ç•™", key="telecom_3"):
        # set_selected_item("å®¢æˆ·è·å–å’Œä¿ç•™")
        set_selected_item("Customer Acquisition and Retention")
    if st.sidebar.button("è®¡è´¹å’Œæ”¶å…¥ç®¡ç†", key="telecom_4"):
        # set_selected_item("è®¡è´¹å’Œæ”¶å…¥ç®¡ç†")
        set_selected_item("Billing and Revenue Management")
    if st.sidebar.button("åˆè§„å’ŒæŠ¥å‘Š", key="telecom_5"):
        # set_selected_item("åˆè§„å’ŒæŠ¥å‘Š")
        set_selected_item("Regulatory Compliance and Reporting")
    st.sidebar.markdown("---")  

    # Healthcare section
    st.sidebar.header("åŒ»ç–—ä¿å¥")  
    if st.sidebar.button("è¯Šæ–­å’Œæ²»ç–—", key="healthcare_1"):
        # set_selected_item("è¯Šæ–­å’Œæ²»ç–—")
        set_selected_item("Diagnosis and Treatment")
    if st.sidebar.button("æŠ¤ç†åè°ƒå’Œç®¡ç†", key="healthcare_2"):
        # set_selected_item("æŠ¤ç†åè°ƒå’Œç®¡ç†")
        set_selected_item("Care Coordination and Management")
    if st.sidebar.button("ç–¾ç—…é¢„é˜²å’Œå¥åº·ä¿ƒè¿›", key="healthcare_3"):
        # set_selected_item("ç–¾ç—…é¢„é˜²å’Œå¥åº·ä¿ƒè¿›")
        set_selected_item("Disease Prevention and Health Promotion")
    if st.sidebar.button("ç ”ç©¶å’Œåˆ›æ–°", key="healthcare_4"):
        # set_selected_item("ç ”ç©¶å’Œåˆ›æ–°")
        set_selected_item("Research and Innovation")
    if st.sidebar.button("åˆè§„å’ŒæŠ¥å‘Š", key="healthcare_5"):
        # set_selected_item("åˆè§„å’ŒæŠ¥å‘Š")
        set_selected_item("Compliance and Reporting")
    st.sidebar.markdown("---")  

    # Education section
    st.sidebar.header("æ•™è‚²")  
    if st.sidebar.button("è¯¾ç¨‹è®¾è®¡å’Œäº¤ä»˜", key="education_1"):
        # set_selected_item("è¯¾ç¨‹è®¾è®¡å’Œäº¤ä»˜")
        set_selected_item("Curriculum Design and Delivery")
    if st.sidebar.button("è¯„ä¼°å’Œè¯„ä»·", key="education_2"):
        # set_selected_item("è¯„ä¼°å’Œè¯„ä»·")
        set_selected_item("Assessment and Evaluation")
    if st.sidebar.button("å­¦ç”Ÿæ”¯æŒå’Œå‚ä¸", key="education_3"):
        # set_selected_item("å­¦ç”Ÿæ”¯æŒå’Œå‚ä¸")
        set_selected_item("Student Support and Engagement")
    if st.sidebar.button("ä¸“ä¸šå‘å±•å’Œåä½œ", key="education_4"):
        # set_selected_item("ä¸“ä¸šå‘å±•å’Œåä½œ")
        set_selected_item("Professional Development and Collaboration")
    if st.sidebar.button("ç®¡ç†å’Œç®¡ç†", key="education_5"):
        set_selected_item("Administration and Management")
        # set_selected_item("ç®¡ç†å’Œç®¡ç†")
    st.sidebar.markdown("---")  

    
    if 'selected_title' not in st.session_state or not st.session_state['selected_title']:
        st.markdown("### æ¦‚è¿°")
        st.markdown("æ­¤å·¥å…·æ—¨åœ¨å¸®åŠ©æ‚¨æ¢ç´¢ OpenAI çš„ o1-preview æ¨¡å‹å’Œ GPT-4o æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚o1 æ˜¯ä¸€ç§æ–°å‹æ¨¡å‹ï¼Œèƒ½å¤Ÿä¸º LLM è§£é”é«˜çº§æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡åœ¨å‰æœŸèŠ±è´¹æ›´å¤šæ—¶é—´æ€è€ƒé—®é¢˜ï¼Œo1 è€ƒè™‘äº†ä¸€ç³»åˆ—è¾¹ç¼˜æƒ…å†µå’Œæ½œåœ¨æƒ…å†µï¼Œä»è€Œå¾—å‡ºæ›´å¥½çš„ç»“è®ºã€‚è¿™æ˜¯ä»¥å»¶è¿Ÿä¸ºä»£ä»·çš„ã€‚\n o1 æœ‰æœ›æ”¹å˜è®¸å¤šè¡Œä¸šï¼Œæ­¤å·¥å…·æ—¨åœ¨è®©æ‚¨æ¢ç´¢è¿™äº›è¡Œä¸šã€‚")
        st.markdown("### ä½¿ç”¨è¯´æ˜")
        st.markdown("ç‚¹å‡»å·¦ä¾§çš„åœºæ™¯ä»¥å¼€å§‹ã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡é€‰æ‹©â€œè‡ªå®šä¹‰åœºæ™¯â€æ¥ä¸Šä¼ æ‚¨è‡ªå·±çš„åœºæ™¯ã€‚")
        if offline_mode=='true':
            st.markdown("### âš ï¸ç¦»çº¿æ¨¡å¼âš ï¸")
            st.markdown("æ­¤å·¥å…·å½“å‰åœ¨ç¦»çº¿æ¨¡å¼ä¸‹è¿è¡Œã€‚æ‚¨ä»ç„¶å¯ä»¥æ¢ç´¢å’Œè¿è¡Œæ‰€æœ‰åœºæ™¯ï¼Œå±•ç¤º GPT-4o å’Œ o1 çš„è¡Œä¸ºã€‚ä½†æ˜¯ï¼Œæ‚¨å°†æ— æ³•ä¿®æ”¹æç¤ºã€ä¸Šä¼ æ–‡ä»¶æˆ–æ·»åŠ æ‚¨è‡ªå·±çš„è‡ªå®šä¹‰åœºæ™¯ã€‚è¦å®æ—¶å°è¯•ï¼Œè¯·åœ¨æœ¬åœ°æ‰˜ç®¡æ­¤åº”ç”¨ç¨‹åºå¹¶ä½¿ç”¨æ‚¨è‡ªå·±çš„ API å¯†é’¥ã€‚è”ç³» Luca.Stamatescu@microsoft.com äº†è§£æ›´å¤šä¿¡æ¯ã€‚")
        st.markdown("### å½’å±")
        st.markdown("è¯·è”ç³» Luca Stamatescu ä»¥è·å–æœ‰å…³æ­¤æ¼”ç¤ºçš„æ›´å¤šä¿¡æ¯ã€‚æ„Ÿè°¢ Salim Naim å¼€å‘æç¤ºç­–ç•¥å’Œ Ibrahim Hamza æä¾›è¡Œä¸šåœºæ™¯å’Œç”¨ä¾‹ã€‚")
    else:
        # Main content
        st.title(st.session_state.get("selected_title", "Custom Scenario"))




        # Custom CSS to hide Streamlit header and footer and adjust padding
        hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            header {visibility: hidden;}
            .css-18e3th9 {padding-top: 0;}
            .css-1d391kg {padding-top: 0;}
            </style>
        """
        st.markdown(hide_streamlit_style, unsafe_allow_html=True)

        
        # Retrieve the selected use case from session state
        selected_use_case = st.session_state.get("selected_title", "Custom Scenario")



        st.markdown("##### é«˜çº§æ¦‚è¿°")
        overview = get_csv_data(selected_use_case,"Overview")

        st.markdown(overview)


        st.markdown("##### è¯¦ç»†åˆ†æ")
        # Get the default input based on the selected use case
        default_input = get_csv_data(selected_use_case,"Prompt")

        # Input box (takes up the width of the screen)   
        #  
        user_input = st.text_area("", value=default_input, height=200)    

        # Section to upload supporting documents
        st.markdown("##### ä¸Šä¼ æ”¯æŒæ–‡æ¡£")
        
        # Use session state to store uploaded files and the uploader key
        if 'uploaded_files' not in st.session_state:
            st.session_state.uploaded_files = None
        if 'uploader_key' not in st.session_state:
            st.session_state.uploader_key = str(randint(1000, 100000000))
        
        # File uploader with a unique key
        uploaded_files = st.file_uploader("é€‰æ‹©å›¾åƒæˆ– PDF", accept_multiple_files=True, type=["jpeg", "pdf"])
        
        # Button to delete uploaded files
        if st.button("åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶"):
            if offline_mode == 'true':
                st.toast(offline_message, icon="âš ï¸")
            else:
                if DELETE_TEMP_FOLDER and os.path.exists(TEMP_FOLDER):
                    shutil.rmtree(TEMP_FOLDER)  
                    st.session_state.descriptions=None
            

        # Process uploaded files
        if uploaded_files and st.button("ä¸Šä¼ æ–‡ä»¶"):
            if offline_mode == 'true':
                st.toast(offline_message, icon="âš ï¸")
            else:
                process_inputs(uploaded_files)
                load_images_and_descriptions("Custom Scenario")
    
        # Display images as tiles with descriptions  
        if 'descriptions' in st.session_state and st.session_state.descriptions!=None:
            # Call the function to render images and descriptions
            render_images_and_descriptions()
    
        # Add a checkbox to toggle the comparison
        compare_models = st.checkbox("ä»…æ˜¾ç¤º o1-preview è¾“å‡º", value=False)
      
        # Button to submit    
        if st.button("Submit"): 
            with st.spinner('å¤„ç†ä¸­...'):
                if st.session_state['descriptions']:  
                    # Ensure descriptions is a string
                    concatenated_descriptions=""
                    descriptions = st.session_state['descriptions']
                    if isinstance(descriptions, list):
                        for description in descriptions:
                            concatenated_descriptions=concatenated_descriptions+description[1]
                    st.session_state['prompt'] = user_input + "\n\n" + concatenated_descriptions
                else:  
                    st.session_state['prompt'] = user_input

                # Conditionally display columns based on the checkbox state
                if not compare_models:
                    col1, col2 = st.columns(2)    
                else:
                    col2 = st.container()
                
                if not compare_models:
                    with col1:    
                        st.subheader("4o å“åº”")
                        st.markdown("---")
                        response_placeholder_4o = st.empty()  
                        st.markdown("---")
                        st.markdown("##### æ—¶é—´")    
                        time_placeholder_4o = st.markdown("å¤„ç†ä¸­...")   
        
                with col2:    
                    st.subheader("o1-preview å“åº”")   
                    st.markdown("---")
                    response_placeholder_o1 = st.empty() 
                    st.markdown("---")
                    st.markdown("##### æ—¶é—´")   
                    time_placeholder_o1 = st.markdown("å¤„ç†ä¸­...")   
                
                # Dictionary to store results    
                result_dict = {}    
                queue = Queue()  
                
                # Start threads for both API calls    
                threads = []    
                t1 = threading.Thread(target=gpt4o_call, args=("You are a helpful AI assistant.", st.session_state['prompt'], result_dict, queue,selected_use_case))    
                t2 = threading.Thread(target=o1_call_simultaneous_handler, args=("You are a helpful AI assistant.", st.session_state['prompt'], result_dict,selected_use_case))    
                threads.append(t1)    
                threads.append(t2)    
                t1.start()    
                t2.start()    
                
                if not compare_models:
                    # Update the Streamlit UI with the streamed response  
                    while t1.is_alive():  
                        while not queue.empty():  
                            response_placeholder_4o.write(queue.get())  
                        time.sleep(0.1)  
                
                # Wait for both threads to complete    
                for t in threads:    
                    t.join()    
                
                # Display the 4o response and elapsed time  
                if not compare_models:
                    with col1:
                        response_placeholder_4o.write(result_dict['4o']['response'])  
                        time_placeholder_4o.write(f"è€—æ—¶: {result_dict['4o']['time']:.2f} ç§’")  
                        if os.getenv('debug_mode') == 'true':
                            save_csv_data(selected_use_case, "gpt4o_time", float(round(result_dict['4o']['time'],2)))
                            save_csv_data(selected_use_case, "gpt4o", result_dict['4o']['response'])


                # Display the O1 response and elapsed time    
                with col2:    
                    response_placeholder_o1.write(result_dict['o1']['response'])   
                    time_placeholder_o1.write(f"è€—æ—¶: {result_dict['o1']['time']:.2f} ç§’")    
                    if os.getenv('debug_mode') == 'true':
                        save_csv_data(selected_use_case, "o1_time", float(round(result_dict['o1']['time'],2)))
                        save_csv_data(selected_use_case, "o1", result_dict['o1']['response'])

            if not compare_models:
                st.markdown("---")
                # Compare the responses and display the comparison  
                st.subheader("å“åº”æ¯”è¾ƒ - æ¦‚è¿°")  

                with st.spinner('å¤„ç†ä¸­...'):
                    if offline_mode == 'true':
                        comparison_result = get_csv_data(selected_use_case, 'simple_comparison')
                        # Simulate a wait time
                        time.sleep(15)
                    else:
                        comparison_result = compare_responses_simple(result_dict['4o']['response'], result_dict['o1']['response'])  
                    st.write(comparison_result)
                    save_csv_data(selected_use_case, "simple_comparison", comparison_result)

                st.markdown("---")
                # Compare the responses and display the comparison  
                st.subheader("å“åº”æ¯”è¾ƒ - è¯¦ç»†")  

                with st.spinner('Processing...'):
                    if offline_mode == 'true':
                        comparison_result = get_csv_data(selected_use_case, 'complex_comparison')
                        # Simulate a wait time
                        time.sleep(15)
                    else:
                        comparison_result = compare_responses(result_dict['4o']['response'], result_dict['o1']['response'])  
                    st.write(comparison_result)
                    save_csv_data(selected_use_case, "complex_comparison", comparison_result)



if __name__ == "__main__":
    main()
